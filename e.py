import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, AutoProcessor
from PIL import Image
import requests
from io import BytesIO
import base64

class KoreanBlossomVision:
    def __init__(self, model_name="Bllossom/llama-3.1-Korean-Bllossom-Vision-8B"):
        """
        í•œêµ­ì–´ Bllossom Vision ëª¨ë¸ ì´ˆê¸°í™”
        í…ìŠ¤íŠ¸ì™€ ì´ë¯¸ì§€ë¥¼ ëª¨ë‘ ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ” ë©€í‹°ëª¨ë‹¬ ëª¨ë¸
        """
        self.model_name = model_name
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        
        print(f"ğŸŒ¸ í•œêµ­ì–´ Bllossom Vision ëª¨ë¸ ë¡œë”© ì¤‘...")
        print(f"ğŸ“± ì‚¬ìš© ë””ë°”ì´ìŠ¤: {self.device}")
        
        self.load_model()
    
    def load_model(self):
        """ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ"""
        try:
            # í† í¬ë‚˜ì´ì € ë¡œë“œ
            self.tokenizer = AutoTokenizer.from_pretrained(
                self.model_name,
                trust_remote_code=True
            )
            
            # í”„ë¡œì„¸ì„œ ë¡œë“œ (ì´ë¯¸ì§€ ì²˜ë¦¬ìš©)
            self.processor = AutoProcessor.from_pretrained(
                self.model_name,
                trust_remote_code=True
            )
            
            # ëª¨ë¸ ë¡œë“œ ì„¤ì •
            model_kwargs = {
                "trust_remote_code": True,
                "torch_dtype": torch.float16 if torch.cuda.is_available() else torch.float32,
            }
            
            # GPU ë©”ëª¨ë¦¬ ìµœì í™”
            if torch.cuda.is_available():
                from transformers import BitsAndBytesConfig
                quantization_config = BitsAndBytesConfig(
                    load_in_4bit=True,
                    bnb_4bit_compute_dtype=torch.float16,
                    bnb_4bit_use_double_quant=True,
                    bnb_4bit_quant_type="nf4"
                )
                model_kwargs["quantization_config"] = quantization_config
                model_kwargs["device_map"] = "auto"
            
            # ëª¨ë¸ ë¡œë“œ
            self.model = AutoModelForCausalLM.from_pretrained(
                self.model_name,
                **model_kwargs
            )
            
            print("âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ!")
            self.print_model_info()
            
        except Exception as e:
            print(f"âŒ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            raise
    
    def print_model_info(self):
        """ëª¨ë¸ ì •ë³´ ì¶œë ¥"""
        param_count = sum(p.numel() for p in self.model.parameters())
        print(f"ğŸ“Š ëª¨ë¸ íŒŒë¼ë¯¸í„° ìˆ˜: {param_count:,}")
        print(f"ğŸŒ¸ íŠ¹ì§•: í•œêµ­ì–´ íŠ¹í™” + ë¹„ì „ ê¸°ëŠ¥")
    
    def load_image(self, image_source):
        """ë‹¤ì–‘í•œ ì†ŒìŠ¤ì—ì„œ ì´ë¯¸ì§€ ë¡œë“œ"""
        if isinstance(image_source, str):
            if image_source.startswith(('http://', 'https://')):
                # URLì—ì„œ ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ
                response = requests.get(image_source)
                image = Image.open(BytesIO(response.content))
            else:
                # ë¡œì»¬ íŒŒì¼ì—ì„œ ì´ë¯¸ì§€ ë¡œë“œ
                image = Image.open(image_source)
        elif isinstance(image_source, Image.Image):
            # PIL Image ê°ì²´
            image = image_source
        else:
            raise ValueError("ì§€ì›í•˜ì§€ ì•ŠëŠ” ì´ë¯¸ì§€ í˜•ì‹ì…ë‹ˆë‹¤.")
        
        # RGBë¡œ ë³€í™˜ (í•„ìš”í•œ ê²½ìš°)
        if image.mode != 'RGB':
            image = image.convert('RGB')
        
        return image
    
    def generate_text(self, prompt: str, max_length: int = 512, temperature: float = 0.7):
        """í…ìŠ¤íŠ¸ë§Œ ì‚¬ìš©í•œ ìƒì„±"""
        # Llama 3.1 í•œêµ­ì–´ ì±„íŒ… í¬ë§·
        formatted_prompt = f"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n\n{prompt}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n"
        
        inputs = self.tokenizer(formatted_prompt, return_tensors="pt").to(self.device)
        
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_length=max_length,
                temperature=temperature,
                do_sample=True,
                top_p=0.9,
                pad_token_id=self.tokenizer.eos_token_id,
                eos_token_id=self.tokenizer.eos_token_id
            )
        
        # ìƒì„±ëœ í…ìŠ¤íŠ¸ì—ì„œ í”„ë¡¬í”„íŠ¸ ë¶€ë¶„ ì œê±°
        response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
        if formatted_prompt in response:
            response = response.replace(formatted_prompt, "").strip()
        
        return response
    
    def analyze_image(self, image_source, question: str = "ì´ ì´ë¯¸ì§€ì— ëŒ€í•´ ìì„¸íˆ ì„¤ëª…í•´ì£¼ì„¸ìš”."):
        """ì´ë¯¸ì§€ ë¶„ì„ ë° ì§ˆë¬¸ ë‹µë³€"""
        try:
            # ì´ë¯¸ì§€ ë¡œë“œ
            image = self.load_image(image_source)
            
            # ë©€í‹°ëª¨ë‹¬ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
            prompt = f"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n\n<image>\n{question}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n"
            
            # ì´ë¯¸ì§€ì™€ í…ìŠ¤íŠ¸ ì²˜ë¦¬
            inputs = self.processor(
                text=prompt,
                images=image,
                return_tensors="pt"
            ).to(self.device)
            
            # ìƒì„±
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_length=1024,
                    temperature=0.7,
                    do_sample=True,
                    top_p=0.9,
                    pad_token_id=self.tokenizer.eos_token_id
                )
            
            # ì‘ë‹µ ë””ì½”ë”©
            response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
            
            # í”„ë¡¬í”„íŠ¸ ë¶€ë¶„ ì œê±°
            if "<|start_header_id|>assistant<|end_header_id|>" in response:
                response = response.split("<|start_header_id|>assistant<|end_header_id|>")[-1].strip()
            
            return response
            
        except Exception as e:
            return f"ì´ë¯¸ì§€ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"
    
    def chat_with_image(self, image_source, conversation_history=None):
        """ì´ë¯¸ì§€ì™€ í•¨ê»˜ ëŒ€í™”í•˜ê¸°"""
        if conversation_history is None:
            conversation_history = []
        
        image = self.load_image(image_source)
        
        print("ğŸ–¼ï¸ ì´ë¯¸ì§€ê°€ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤. ì´ë¯¸ì§€ì— ëŒ€í•´ ì§ˆë¬¸í•´ë³´ì„¸ìš”!")
        print("ğŸ’¡ ì˜ˆì‹œ: 'ì´ ì´ë¯¸ì§€ì—ì„œ ë¬´ì—‡ì„ ë³¼ ìˆ˜ ìˆë‚˜ìš”?', 'ì´ ì‚¬ëŒì˜ í‘œì •ì€ ì–´ë–¤ê°€ìš”?'")
        print("â¹ï¸ 'quit'ì„ ì…ë ¥í•˜ë©´ ì¢…ë£Œë©ë‹ˆë‹¤.\n")
        
        while True:
            user_input = input("ğŸ‘¤ ì§ˆë¬¸: ").strip()
            
            if user_input.lower() in ['quit', 'exit', 'ì¢…ë£Œ']:
                break
            
            if not user_input:
                continue
            
            print("ğŸ¤– ë¶„ì„ ì¤‘...", end="", flush=True)
            
            try:
                response = self.analyze_image(image, user_input)
                print(f"\rğŸŒ¸ Bllossom: {response}\n")
                
                # ëŒ€í™” íˆìŠ¤í† ë¦¬ì— ì¶”ê°€
                conversation_history.append({"user": user_input, "assistant": response})
                
            except Exception as e:
                print(f"\râŒ ì˜¤ë¥˜: {e}\n")
    
    def batch_image_analysis(self, image_list, questions=None):
        """ì—¬ëŸ¬ ì´ë¯¸ì§€ ë°°ì¹˜ ë¶„ì„"""
        if questions is None:
            questions = ["ì´ ì´ë¯¸ì§€ì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”."] * len(image_list)
        
        if len(questions) == 1:
            questions = questions * len(image_list)
        
        results = []
        
        for i, (image_source, question) in enumerate(zip(image_list, questions), 1):
            print(f"ğŸ”„ ì´ë¯¸ì§€ ë¶„ì„ ì¤‘ ({i}/{len(image_list)})...")
            
            try:
                result = self.analyze_image(image_source, question)
                results.append({
                    "image": image_source,
                    "question": question,
                    "analysis": result
                })
            except Exception as e:
                results.append({
                    "image": image_source,
                    "question": question,
                    "error": str(e)
                })
        
        return results
    
    def korean_conversation(self):
        """í•œêµ­ì–´ í…ìŠ¤íŠ¸ ëŒ€í™”"""
        print("ğŸŒ¸ í•œêµ­ì–´ Bllossomê³¼ ëŒ€í™”í•´ë³´ì„¸ìš”!")
        print("ğŸ’¡ ì´ ëª¨ë¸ì€ í•œêµ­ì–´ì— íŠ¹ë³„íˆ ìµœì í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
        print("â¹ï¸ 'quit'ì„ ì…ë ¥í•˜ë©´ ì¢…ë£Œë©ë‹ˆë‹¤.\n")
        
        while True:
            user_input = input("ğŸ‘¤ You: ").strip()
            
            if user_input.lower() in ['quit', 'exit', 'ì¢…ë£Œ']:
                break
            
            if not user_input:
                continue
            
            print("ğŸ¤– ìƒê° ì¤‘...", end="", flush=True)
            
            try:
                response = self.generate_text(user_input)
                print(f"\rğŸŒ¸ Bllossom: {response}\n")
            except Exception as e:
                print(f"\râŒ ì˜¤ë¥˜: {e}\n")

# ì‚¬ìš© ì˜ˆì‹œ ë° í…ŒìŠ¤íŠ¸ ì½”ë“œ
def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸŒ¸ í•œêµ­ì–´ Bllossom Vision ëª¨ë¸ í…ŒìŠ¤íŠ¸")
    print("=" * 40)
    
    try:
        # ëª¨ë¸ ì´ˆê¸°í™”
        bllossom = KoreanBlossomVision()
        
        print("\nğŸ¯ ì‚¬ìš© ë°©ë²•ì„ ì„ íƒí•˜ì„¸ìš”:")
        print("1. í•œêµ­ì–´ í…ìŠ¤íŠ¸ ëŒ€í™”")
        print("2. ì´ë¯¸ì§€ ë¶„ì„ (URL ë˜ëŠ” íŒŒì¼)")
        print("3. ì´ë¯¸ì§€ì™€ ëŒ€í™”í•˜ê¸°")
        print("4. ë°°ì¹˜ ì´ë¯¸ì§€ ë¶„ì„")
        print("5. ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸")
        
        choice = input("\nì„ íƒ (1-5): ").strip()
        
        if choice == "1":
            bllossom.korean_conversation()
        
        elif choice == "2":
            image_path = input("ì´ë¯¸ì§€ URL ë˜ëŠ” íŒŒì¼ ê²½ë¡œë¥¼ ì…ë ¥í•˜ì„¸ìš”: ").strip()
            question = input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš” (ì—”í„°ë¡œ ê¸°ë³¸ ì§ˆë¬¸): ").strip()
            
            if not question:
                question = "ì´ ì´ë¯¸ì§€ì— ëŒ€í•´ ìì„¸íˆ ì„¤ëª…í•´ì£¼ì„¸ìš”."
            
            print("\nğŸ”„ ì´ë¯¸ì§€ ë¶„ì„ ì¤‘...")
            result = bllossom.analyze_image(image_path, question)
            print(f"\nğŸŒ¸ ë¶„ì„ ê²°ê³¼:\n{result}")
        
        elif choice == "3":
            image_path = input("ì´ë¯¸ì§€ URL ë˜ëŠ” íŒŒì¼ ê²½ë¡œë¥¼ ì…ë ¥í•˜ì„¸ìš”: ").strip()
            bllossom.chat_with_image(image_path)
        
        elif choice == "4":
            print("ë°°ì¹˜ ë¶„ì„ì„ ìœ„í•´ ì´ë¯¸ì§€ URLë“¤ì„ ì…ë ¥í•˜ì„¸ìš” (ë¹ˆ ì¤„ë¡œ ì¢…ë£Œ):")
            images = []
            while True:
                img_url = input(f"ì´ë¯¸ì§€ {len(images)+1}: ").strip()
                if not img_url:
                    break
                images.append(img_url)
            
            if images:
                results = bllossom.batch_image_analysis(images)
                print("\nğŸ“‹ ë°°ì¹˜ ë¶„ì„ ê²°ê³¼:")
                for i, result in enumerate(results, 1):
                    print(f"\n{i}. ì´ë¯¸ì§€: {result['image']}")
                    if 'analysis' in result:
                        print(f"   ë¶„ì„: {result['analysis'][:100]}...")
                    else:
                        print(f"   ì˜¤ë¥˜: {result['error']}")
        
        elif choice == "5":
            # ê°„ë‹¨í•œ í…ìŠ¤íŠ¸ í…ŒìŠ¤íŠ¸
            print("\nğŸ§ª ê°„ë‹¨í•œ í•œêµ­ì–´ í…ŒìŠ¤íŠ¸:")
            test_prompt = "ì•ˆë…•í•˜ì„¸ìš”! í•œêµ­ì˜ ì „í†µ ìŒì‹ì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”."
            result = bllossom.generate_text(test_prompt)
            print(f"ì§ˆë¬¸: {test_prompt}")
            print(f"ë‹µë³€: {result}")
    
    except KeyboardInterrupt:
        print("\n\nâ¹ï¸ í”„ë¡œê·¸ë¨ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")

# ìƒ˜í”Œ ì´ë¯¸ì§€ URLë“¤ (í…ŒìŠ¤íŠ¸ìš©)
SAMPLE_IMAGES = [
    "https://upload.wikimedia.org/wikipedia/commons/thumb/4/47/PNG_transparency_demonstration_1.png/280px-PNG_transparency_demonstration_1.png",
    "https://upload.wikimedia.org/wikipedia/commons/thumb/5/50/Vd-Orig.png/256px-Vd-Orig.png"
]

# íŠ¹í™” ê¸°ëŠ¥ë“¤
def korean_cooking_assistant():
    """í•œêµ­ ìš”ë¦¬ ë„ìš°ë¯¸"""
    bllossom = KoreanBlossomVision()
    
    print("ğŸ³ í•œêµ­ ìš”ë¦¬ ë„ìš°ë¯¸ì…ë‹ˆë‹¤!")
    print("ìŒì‹ ì‚¬ì§„ì„ ë³´ì—¬ì£¼ì‹œë©´ ë ˆì‹œí”¼ë‚˜ ìš”ë¦¬ë²•ì„ ì•Œë ¤ë“œë ¤ìš”.")
    
    while True:
        image_path = input("\nìŒì‹ ì‚¬ì§„ URLì„ ì…ë ¥í•˜ì„¸ìš” (quitìœ¼ë¡œ ì¢…ë£Œ): ").strip()
        if image_path.lower() == 'quit':
            break
        
        try:
            result = bllossom.analyze_image(
                image_path, 
                "ì´ ìŒì‹ì˜ ì´ë¦„ê³¼ ë§Œë“œëŠ” ë°©ë²•ì„ í•œêµ­ì–´ë¡œ ìì„¸íˆ ì„¤ëª…í•´ì£¼ì„¸ìš”."
            )
            print(f"\nğŸŒ¸ ìš”ë¦¬ ë¶„ì„:\n{result}")
        except Exception as e:
            print(f"âŒ ë¶„ì„ ì˜¤ë¥˜: {e}")

def document_reader():
    """ë¬¸ì„œ ì´ë¯¸ì§€ ì½ê¸°"""
    bllossom = KoreanBlossomVision()
    
    print("ğŸ“„ ë¬¸ì„œ ì´ë¯¸ì§€ ì½ê¸° ë„êµ¬ì…ë‹ˆë‹¤!")
    print("ë¬¸ì„œë‚˜ í…ìŠ¤íŠ¸ê°€ í¬í•¨ëœ ì´ë¯¸ì§€ë¥¼ ë¶„ì„í•´ë“œë ¤ìš”.")
    
    while True:
        image_path = input("\në¬¸ì„œ ì´ë¯¸ì§€ URLì„ ì…ë ¥í•˜ì„¸ìš” (quitìœ¼ë¡œ ì¢…ë£Œ): ").strip()
        if image_path.lower() == 'quit':
            break
        
        try:
            result = bllossom.analyze_image(
                image_path,
                "ì´ ì´ë¯¸ì§€ì— ìˆëŠ” í…ìŠ¤íŠ¸ë¥¼ ëª¨ë‘ ì½ì–´ì„œ í•œêµ­ì–´ë¡œ ì •ë¦¬í•´ì£¼ì„¸ìš”."
            )
            print(f"\nğŸ“ ë¬¸ì„œ ë‚´ìš©:\n{result}")
        except Exception as e:
            print(f"âŒ ì½ê¸° ì˜¤ë¥˜: {e}")

if __name__ == "__main__":
    main()
    
    # ì¶”ê°€ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸
    print("\nğŸŒ¸ ì¶”ê°€ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸:")
    korean_cooking_assistant()
    document_reader()
    
    print("\nğŸŒ¸ í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤. ê°ì‚¬í•©ë‹ˆë‹¤!")